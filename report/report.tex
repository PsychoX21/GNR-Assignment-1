\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}

\geometry{margin=1in}

\title{\textbf{GNR638: Machine Learning for Remote Sensing - II}\\
\large Programming Assignment 1: Design a Deep Learning Framework}
\author{DeepNet Framework Report}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This report documents the design, implementation, and evaluation of \textbf{DeepNet}, a custom deep learning framework built from first principles. The primary objective of this project was to demystify the internal mechanics of modern deep learning libraries by constructing a functional equivalent that supports tensor operations, automatic differentiation, and essential neural network layers (Convolution, Pooling, Fully Connected) without relying on external black-box libraries like PyTorch or TensorFlow.

To ensure high performance, the core computational backend is implemented in C++17, leveraging manual memory management and OpenMP for parallelism. This backend is exposed to a Python 3.12 frontend via \texttt{pybind11}, providing a user-friendly API that mirrors established conventions. The framework was successfully used to train ResNet-style architectures on the MNIST and CIFAR-100 datasets, achieving competitive accuracy within strict time and resource constraints.

\section{Framework Design}

\subsection{Backend Implementation (C++)}
The performance-critical components are implemented in C++, focusing on efficiency and explicit resource management.
\begin{itemize}
    \item \textbf{Tensor Class}: A multi-dimensional strided array implementation supporting standard arithmetic operations. It serves as the fundamental data structure, handling data ownership and device placement (CPU/GPU).
    \item \textbf{Memory Management}: The system uses a reference-counting mechanism (via smart pointers) to ensure efficient memory handling. This prevents memory leaks while avoiding the overhead of garbage collection, crucial for training large models.
    \item \textbf{Parallelization}: CPU operations are accelerated using OpenMP directives, allowing for multi-threaded matrix multiplications and convolutions.
    \item \textbf{CUDA Support}: A dedicated CUDA backend works in tandem with the C++ host code, offloading compute-intensive kernels (like GEMM and `im2col` convolutions) to the GPU.
\end{itemize}

\subsection{Frontend API (Python) \& Autograd}
The Python frontend provides a declarative API similar to PyTorch, abstracting the low-level complexities.
\begin{itemize}
    \item \textbf{Module Abstraction}: A base \texttt{Module} class tracks learnable parameters and supports hierarchical model definition through nested sub-modules.
    \item \textbf{Automatic Differentiation}: The framework implements a reverse-mode automatic differentiation (autograd) engine. A dynamic computation graph is constructed during the forward pass. During the backward pass, gradients are propagated via the chain rule, with each topological node invoking its specific `backward` implementation.
    \item \textbf{Data Loading}: An \texttt{ImageFolderDataset} class handles image loading using OpenCV. To mitigate I/O bottlenecks, it implements a hybrid preloading strategy where images are resized and cached in RAM, with on-the-fly augmentations applied during retrieval.
\end{itemize}

\section{Model Architecture}
We designed a custom \textbf{DeepResNet-20} architecture tailored for $32 \times 32$ input resolution.

\subsection{Design Rationale}
Standard architectures like ResNet-18 or VGG-16 are often designed for ImageNet ($224 \times 224$). Naively applying them to CIFAR-100 leads to excessive downsampling (resulting in $1 \times 1$ feature maps too early) and massive parameter counts. Our custom design addresses this:
\begin{itemize}
    \item \textbf{Residual Connections}: Essential for training deep networks, allowing gradients to flow unimpeded through skip connections.
    \item \textbf{Global Average Pooling}: We replaced the parameter-heavy fully connected layers of traditional CNNs with a Global Average Pooling layer. This reduces the parameter count significantly and minimizes overfitting.
    \item \textbf{Strided Convolutions}: Downsampling is performed via stride-2 convolutions in the first layer of each new stage, preserving information better than aggressive max-pooling.
\end{itemize}

\subsection{Architecture Specification}

\subsubsection{MNIST Model (ResNet-20, 1 Channel)}
Optimized for single-channel inputs.
\begin{itemize}
    \item \textbf{Stem}: Conv2D ($3 \times 3$, 16 filters, Stride 1, Pad 1) $\to$ BatchNorm $\to$ ReLU
    \item \textbf{Stage 1}: $3 \times$ Residual Blocks (16 filters)
    \item \textbf{Stage 2}: $3 \times$ Residual Blocks (32 filters, Stride 2)
    \item \textbf{Stage 3}: $3 \times$ Residual Blocks (64 filters, Stride 2)
    \item \textbf{Head}: GlobalAvgPool $\to$ Linear (64 $\to$ 10)
    \item \textbf{Total Parameters}: $\approx 0.27$ Million
\end{itemize}

\subsubsection{CIFAR-100 Model (ResNet-20, 3 Channels)}
Optimized for 3-channel RGB inputs.
\begin{itemize}
    \item \textbf{Stem}: Conv2D ($3 \times 3$, 16 filters, Stride 1, Pad 1) $\to$ BatchNorm $\to$ ReLU
    \item \textbf{Stage 1}: $3 \times$ Residual Blocks (16 filters)
    \item \textbf{Stage 2}: $3 \times$ Residual Blocks (32 filters, Stride 2)
    \item \textbf{Stage 3}: $3 \times$ Residual Blocks (64 filters, Stride 2)
    \item \textbf{Head}: GlobalAvgPool $\to$ Linear (64 $\to$ 100)
    \item \textbf{Total Parameters}: $\approx 0.28$ Million
\end{itemize}

\section{Implementation Details}
\subsection{Dataset Loading}
Images are loaded using OpenCV. To meet the performance requirements:
\begin{itemize}
    \item \textbf{Hybrid Preloading}: Images are resized to $32 \times 32$ and stored in RAM. This trades memory (approx. 200MB for CIFAR-100) for significant speedups during training, eliminating disk I/O latency.
    \item \textbf{Augmentation}: Random crops (padding + crop), horizontal flips, and color jitters are applied dynamically in \texttt{\_\_getitem\_\_}. This acts as a regularizer, preventing the model from memorizing the exact pixel values of the training set.
\end{itemize}

\section{Experimental Results}

\subsection{Model Complexity \& Efficiency}
The framework calculates MACs (Multiply-Accumulate operations) and FLOPs.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{MNIST (Data 1)} & \textbf{CIFAR-100 (Data 2)} \\
\midrule
\textbf{Loading Time} & 11.02 seconds & 8.29 seconds \\
\textbf{Parameters} & 272,186 & 278,324 \\
\textbf{MACs} & 5.19 G & 5.22 G \\
\textbf{FLOPs} & 10.37 G & 10.45 G \\
\bottomrule
\end{tabular}
\caption{Efficiency Metrics}
\end{table}

\subsection{Training Performance}
We trained the models using Stochastic Gradient Descent (SGD) with momentum (0.9) and weight decay ($5e-4$). A Cosine Annealing learning rate scheduler was employed to smoothly decay the learning rate, helping the model settle into flatter minima.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/MNIST_real_loss.png}
    \caption{Training and Validation Loss for MNIST}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/MNIST_real_acc.png}
    \caption{Training and Validation Accuracy for MNIST. The model rapidly converges to $>98\%$ accuracy within 5 epochs.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/CIFAR100_real_loss.png}
    \caption{Training and Validation Loss for CIFAR-100}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/CIFAR100_real_acc.png}
    \caption{Training and Validation Accuracy for CIFAR-100. The divergence between training and validation accuracy after epoch 50 indicates the capacity of the model to overfit, which was mitigated by data augmentation.}
\end{figure}

\section{Failed Design Decisions}

\subsection{Initial Attempt: ResNet-18}
\textbf{Design:} We initially attempted to implement the standard ResNet-18 architecture (4 stages, [2, 2, 2, 2] blocks, starting with 64 filters).

\textbf{Issue:}
\begin{enumerate}
    \item \textbf{Training Time}: The model was computationally too expensive.
    \item \textbf{Overfitting}: With 11 million parameters, the model severely overfitted the small $32 \times 32$ dataset.
\end{enumerate}

\textbf{Resolution:}
We switched to the \textbf{ResNet-20} variant. This reduced parameters from $\sim 11M$ to $\sim 0.27M$, increasing validation accuracy to $>60\%$ and minimizing computational load.

\section{Limitations \& Future Work}
While DeepNet is functional, several areas exist for improvement:
\begin{itemize}
    \item \textbf{Optimizer Support}: Currently limited to SGD and Adam. Implementing adaptive optimizers like AdamW could improve convergence on complex tasks.
    \item \textbf{Distributed Training}: The framework is limited to single-GPU training. Adding MPI support for distributed data parallel training would allow scaling to larger datasets.
    \item \textbf{Dynamic Graphs}: While we support dynamic graphs, the memory overhead is higher than static graph frameworks. Implementing graph optimization / fusion passes could reduce memory footprint.
\end{itemize}

\section{AI Usage Declaration}
In compliance with the assignment honor code, we explicitly declare the use of AI assistance (specifically \textbf{Antigravity AI}) in the development of this framework. This support was strictly limited to engineering and optimization tasks, while the core design decisions and architectural logic remained with the authors.

\subsection{Areas of AI Assistance}
\begin{itemize}
    \item \textbf{C++ Backend Modularization}: AI was used to refactor the initial monolithic C++ code into clean, modular components (header/source separation) to improve maintainability.
    \item \textbf{CUDA Integration}: The complex boilerplate required for the CUDA extension (kernel launches, memory management, and `pybind11` glue code) was generated with AI assistance to ensure correctness and performance.
    \item \textbf{Layer \& Optimizer Expansion}: AI assisted in extending the framework's core library with additional layer types and optimizers, ensuring they were correctly vectorized and integrated with the autograd engine.
    \item \textbf{Build System Optimization}: The `CMakeLists.txt` and `Makefile` were optimized by AI to support cross-platform compilation (Windows/Linux) and automatic dependency handling.
    \item \textbf{Debugging}: AI helped diagnose and fix obscure segmentation faults related to memory management in the C++ backend.
    \item \textbf{Report Generation}: AI assisted in structuring this report, generating the LaTeX template, and summarizing the technical details to ensure professional formatting and clarity.
\end{itemize}

\subsection{Student Contributions}
The following aspects were designed and implemented primarily by the student:
\begin{enumerate}
    \item \textbf{Framework Architecture}: The decision to use a dynamic computation graph (autograd) and the specific class hierarchy (Module, Tensor, Optimizer).
    \item \textbf{Model Design}: The rationale behind choosing the ResNet-20 architecture, including the specific channel widths and pooling strategies for $32 \times 32$ data.
    \item \textbf{Training Logic}: The implementation of the training loops, data augmentation pipelines, and hyperparameter tuning.
    \item \textbf{Bug Fixing}: Identifying and resolving high-level logic errors in the training process.
\end{enumerate}

\section{Conclusion}
The \textbf{DeepNet} framework successfully implements a functional deep learning library from scratch. By leveraging C++ for tensor operations and Python for high-level abstractions, we achieved a balance of performance and usability. The project demonstrates a successful integration of low-level optimization (CUDA, C++) with high-level API design (Python, Autograd), meeting all assignment objectives.

\section{References}
\begin{itemize}
    \item \textbf{MNIST Benchmark}: \url{https://www.kaggle.com/code/paulbacher/mnist-99-6-accuracy-top-10-leaderboard}
    \item \textbf{ResNet-20 on CIFAR-100}: \url{https://www.kaggle.com/code/nikitabreskanu/resnet-20-on-cifar-100}
    \item \textbf{Deep Residual Learning for Image Recognition} (He et al., 2016): Foundation for the ResNet architecture used.
    \item \textbf{Pybind11 Documentation}: \url{https://pybind11.readthedocs.io/}
    \item \textbf{Course Materials}: GNR638 Lecture Notes on Backpropagation and CNNs.
\end{itemize}

\end{document}
