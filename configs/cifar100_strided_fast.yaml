# CIFAR-100 Fast Strided Config
# Optimized for high speed (~10-15m/epoch) on limited compute.
# Key idea: Use stride=2 convolutions instead of MaxPool.
# Strided convs compute only pixels needed for the next layer (1/4th the work of Conv+Pool).
#
# Architecture:
# Input: 32x32x3
# 1. Conv 3x3, stride=2 -> 16x16x32   (Fast downsample)
# 2. Conv 3x3, stride=2 -> 8x8x64     (Fast downsample)
# 3. Conv 3x3, stride=2 -> 4x4x128    (Fast downsample)
# 4. FC 2048 -> 512
# 5. FC 512 -> 100
#
# Total Params: ~1.2M
# MACs/sample: ~2.6M (Comparable to MNIST LeNet's ~1.4M)

model:
  name: "CIFAR100_Strided_Fast"
  architecture:
    # Block 1: 32x32 -> 16x16
    - type: "Conv2D"
      in_channels: 3
      out_channels: 32
      kernel_size: 3
      stride: 2
      padding: 1
    
    - type: "BatchNorm2D"
      num_features: 32
    
    - type: "ReLU"
    
    # Block 2: 16x16 -> 8x8
    - type: "Conv2D"
      in_channels: 32
      out_channels: 64
      kernel_size: 3
      stride: 2
      padding: 1
      
    - type: "BatchNorm2D"
      num_features: 64
      
    - type: "ReLU"
    
    # Block 3: 8x8 -> 4x4
    - type: "Conv2D"
      in_channels: 64
      out_channels: 128
      kernel_size: 3
      stride: 2
      padding: 1
      
    - type: "BatchNorm2D"
      num_features: 128
      
    - type: "ReLU"
    
    # Classifier
    - type: "Flatten"
    
    - type: "Linear"
      in_features: 2048  # 128 * 4 * 4
      out_features: 512
      
    - type: "ReLU"
    
    - type: "Dropout"
      p: 0.3
      
    - type: "Linear"
      in_features: 512
      out_features: "num_classes"

training:
  epochs: 30
  batch_size: 128        
  learning_rate: 0.005
  optimizer: "Adam"
  weight_decay: 0.0001
  
  scheduler:
    type: "CosineAnnealingLR"
    T_max: 30
    eta_min: 0.00001

data:
  image_size: 32
  channels: 3
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    rotation: 12

device:
  use_cuda: true
  device_id: 0
