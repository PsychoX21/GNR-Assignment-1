# MNIST (data_1) - Classic LeNet Config
# Simple and fast: Conv(32) -> Pool -> Conv(64) -> Pool -> FC(128) -> FC(10)
# Added BatchNorm for faster convergence

# Spatial: 28 -> Conv(pad=1) -> 28 -> Pool -> 14 -> Conv(pad=1) -> 14 -> Pool -> 7
# Flatten: 64 * 7 * 7 = 3136

model:
  name: "MNIST_LeNet"
  architecture:
    # Block 1: 1 -> 32
    - type: "Conv2D"
      in_channels: 1
      out_channels: 32
      kernel_size: 3
      stride: 1
      padding: 1
    
    - type: "BatchNorm2D"
      num_features: 32
    
    - type: "ReLU"
    
    - type: "MaxPool2D"
      kernel_size: 2
      stride: 2
    # Now 14x14
    
    # Block 2: 32 -> 64
    - type: "Conv2D"
      in_channels: 32
      out_channels: 64
      kernel_size: 3
      stride: 1
      padding: 1
    
    - type: "BatchNorm2D"
      num_features: 64
    
    - type: "ReLU"
    
    - type: "MaxPool2D"
      kernel_size: 2
      stride: 2
    # Now 7x7
    
    # Classifier
    - type: "Flatten"
    
    - type: "Linear"
      in_features: 3136  # 64 * 7 * 7
      out_features: 128
    
    - type: "ReLU"
    
    - type: "Dropout"
      p: 0.3
    
    - type: "Linear"
      in_features: 128
      out_features: "num_classes"

training:
  epochs: 30
  batch_size: 128
  learning_rate: 0.001
  optimizer: "Adam"
  weight_decay: 0.0001
  
  scheduler:
    type: "StepLR"
    step_size: 10
    gamma: 0.5

data:
  image_size: 28
  channels: 1  # Grayscale
  augmentation:
    enabled: false

device:
  use_cuda: true
  device_id: 0
