# CIFAR-100 Slim ResNet Config
# Minimal residual network: 4 ResBlocks (one per stage) with narrow channels
# ~280K params vs 2.8M in the full version â€” 10x lighter
# Spatial: 32 -> 32 -> 16 -> 8 -> 4 -> GAP -> FC(100)

model:
  name: "ResNet_Slim_CIFAR100"
  architecture:
    # Initial conv: 3 -> 16, keeps 32x32
    - type: "Conv2D"
      in_channels: 3
      out_channels: 16
      kernel_size: 3
      stride: 1
      padding: 1

    - type: "BatchNorm2D"
      num_features: 16

    - type: "ReLU"

    # Stage 1 (32x32): 16 -> 32, downsample
    - type: "ResidualBlock"
      in_channels: 16
      out_channels: 32
      stride: 2

    # Stage 2 (16x16): 32 -> 64, downsample
    - type: "ResidualBlock"
      in_channels: 32
      out_channels: 64
      stride: 2

    # Stage 3 (8x8): 64 -> 128, downsample
    - type: "ResidualBlock"
      in_channels: 64
      out_channels: 128
      stride: 2

    # Stage 4 (4x4): 128 -> 128, same
    - type: "ResidualBlock"
      in_channels: 128
      out_channels: 128
      stride: 1

    # Global Average Pooling: [batch, 128, 4, 4] -> [batch, 128]
    - type: "GlobalAveragePooling2D"

    # Classifier
    - type: "Linear"
      in_features: 128
      out_features: "num_classes"

training:
  epochs: 50
  batch_size: 128
  learning_rate: 0.01
  optimizer: "SGD"
  momentum: 0.9
  weight_decay: 0.0005

  scheduler:
    type: "CosineAnnealingLR"
    T_max: 50
    eta_min: 0.0001

data:
  image_size: 32
  channels: 3
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    rotation: 10

device:
  use_cuda: true
  device_id: 0
