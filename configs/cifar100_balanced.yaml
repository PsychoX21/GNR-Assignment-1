# CIFAR-100 Balanced Config
# Optimized for accuracy within 3 hours (target ~20 epochs).
# Uses Residual Blocks with Stride=2 for efficient downsampling.
# Deeper than "Fast" config, but still avoids expensive MaxPool.
#
# Architecture (ResNet-10 style):
# 1. Conv 3x3 (3->64)
# 2. ResBlock (64->64)
# 3. ResBlock (64->128, stride=2)  -> 16x16
# 4. ResBlock (128->256, stride=2) -> 8x8
# 5. ResBlock (256->512, stride=2) -> 4x4
# 6. GlobalAvgPool -> 512
# 7. FC 512 -> 100
#
# Params: ~4.5M
# MACs: Higher than Fast, but better accuracy.

model:
  name: "CIFAR100_Balanced"
  architecture:
    # Initial Conv
    - type: "Conv2D"
      in_channels: 3
      out_channels: 32
      kernel_size: 3
      stride: 1
      padding: 1
      
    - type: "BatchNorm2D"
      num_features: 32
      
    - type: "ReLU"
    
    # Stage 1: 32x32
    - type: "ResidualBlock"
      in_channels: 32
      out_channels: 32
      stride: 1
      
    # Stage 2: 16x16 (Downsample)
    - type: "ResidualBlock"
      in_channels: 32
      out_channels: 64
      stride: 2
      
    # Stage 3: 8x8 (Downsample)
    - type: "ResidualBlock"
      in_channels: 64
      out_channels: 128
      stride: 2
      
    # Stage 4: 4x4 (Downsample)
    - type: "ResidualBlock"
      in_channels: 128
      out_channels: 256
      stride: 2
      
    # Global Pooling
    - type: "GlobalAveragePooling2D"
    
    # Classifier
    - type: "Linear"
      in_features: 256
      out_features: "num_classes"

training:
  epochs: 20
  batch_size: 128        
  learning_rate: 0.1     # Standard SGD LR
  optimizer: "SGD"
  momentum: 0.9
  weight_decay: 0.0005
  
  scheduler:
    type: "CosineAnnealingLR"
    T_max: 20
    eta_min: 0.001

data:
  image_size: 32
  channels: 3
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    rotation: 15

device:
  use_cuda: true
  device_id: 0
